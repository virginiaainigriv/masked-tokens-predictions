{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QffuLiws-j5M"
      },
      "outputs": [],
      "source": [
        "! pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "XWpy30lv-tPD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the files and read their content\n",
        "with open('/content/Plato (0059) - Timaeus (031)_.txt', 'r') as f:\n",
        "    original = f.readlines()\n",
        "with open('/content/Plato (0059) - Timaeus (031)_mask.txt', 'r') as f:\n",
        "    input = f.readlines()"
      ],
      "metadata": {
        "id": "DxAfAiEA_KO6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = ['mask_value', \n",
        "       'predicted_mask_1', 'score_prediction_1', 'cosine_similarity_1', \n",
        "       'predicted_mask_2', 'score_prediction_2', 'cosine_similarity_2',\n",
        "       'predicted_mask_3', 'score_prediction_3', 'cosine_similarity_3',\n",
        "       ]\n",
        "df1 = pd.DataFrame(columns=col)\n",
        "\n",
        "for i, sentence in enumerate(input):\n",
        "    # find position of [MASK] token\n",
        "    try:\n",
        "      mask_pos = sentence.index('[MASK]')\n",
        "       # find true mask token in y_hat\n",
        "      true_value = original[i][mask_pos:]\n",
        "      separators = ' .,;:'\n",
        "      for char in true_value:\n",
        "          if char in separators:\n",
        "              true_value = true_value[:true_value.index(char)]\n",
        "              break\n",
        "\n",
        "      # add to dataframe\n",
        "      df1 = df1.append({\n",
        "          'mask_value': true_value\n",
        "      }, ignore_index=True)\n",
        "    except:\n",
        "      print(sentence)\n",
        "    \n",
        "  "
      ],
      "metadata": {
        "id": "GpMqsZKtTnWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "df1"
      ],
      "metadata": {
        "id": "CBbPTNuqgsxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ag_model = 'pranaydeeps/Ancient-Greek-BERT'\n",
        "pr_model = 'Sonnenblume/bert-base-uncased-ancient-greek-v4'\n",
        "my_model = 'Sonnenblume/bert-base-uncased-ancient-greek-v3'\n",
        "mg_model = 'nlpaueb/bert-base-greek-uncased-v1'\n",
        "\n",
        "\n",
        "model_name = my_model"
      ],
      "metadata": {
        "id": "_uM7gXJzV9IX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "p = pipeline(\n",
        "  \"fill-mask\",\n",
        "  model=model_name,\n",
        "  tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "for i, sentence in enumerate(input):\n",
        "\n",
        "    if i%50 == 0:\n",
        "      print(f'{i}/{len(input)}')\n",
        "\n",
        "    try:    \n",
        "      pred = p(sentence)\n",
        "\n",
        "    \n",
        "      # Replace values of corresponding row in df1\n",
        "      df1.at[i, 'predicted_mask_1'] = pred[0][\"token_str\"]\n",
        "      df1.at[i, 'predicted_mask_2'] = pred[1][\"token_str\"]\n",
        "      df1.at[i, 'predicted_mask_3'] = pred[2][\"token_str\"]\n",
        "\n",
        "      df1.at[i, 'score_prediction_1'] = pred[0][\"score\"]\n",
        "      df1.at[i, 'score_prediction_2'] = pred[1][\"score\"]\n",
        "      df1.at[i, 'score_prediction_3'] = pred[2][\"score\"]\n",
        "\n",
        "      df1.at[i, 'cosine_similarity_1'] = calculate_cosine_similarity(model, tokenizer, df1.at[i, 'mask_value'], pred[0][\"token_str\"], device=device)\n",
        "      df1.at[i, 'cosine_similarity_2'] = calculate_cosine_similarity(model, tokenizer, df1.at[i, 'mask_value'], pred[1][\"token_str\"], device=device)\n",
        "      df1.at[i, 'cosine_similarity_3'] = calculate_cosine_similarity(model, tokenizer, df1.at[i, 'mask_value'], pred[2][\"token_str\"], device=device)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "      print(e)"
      ],
      "metadata": {
        "id": "O1noCX2y_CGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_sorted = df1.sort_values(by=['score_prediction_1'], ascending=[False])\n",
        "\n",
        "df1_sorted\n",
        "\n",
        "df1_sorted.to_csv(f'{model_name}_Plato (0059) - Timaeus (031).csv')"
      ],
      "metadata": {
        "id": "i0eZyBxLg_Xp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_subword_prefix(subword):\n",
        "    if subword.startswith(\"##\"):\n",
        "        return subword[2:]\n",
        "    else:\n",
        "        return \" \" + subword"
      ],
      "metadata": {
        "id": "HgneZJosAofx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(tokenizer, model, sentence):\n",
        "   # tokenize the sentence and find the index of the masked token\n",
        "    tokenized_text = get_tokens(tokenizer, sentence)\n",
        "    masked_index = tokenized_text.index('[MASK]')\n",
        "\n",
        "    # convert the tokenized sentence to input ids and create a tensor of input ids\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    input_ids = torch.tensor([input_ids])\n",
        "\n",
        "    # use the model to make predictions on the masked token\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predictions = outputs[0][0, masked_index].topk(k=3, sorted=True)\n",
        "        predicted_token_indices = predictions.indices.tolist()\n",
        "        predicted_token_logits = predictions.values.tolist()\n",
        "        predicted_token_probs = torch.softmax(torch.tensor(predicted_token_logits), dim=0).tolist()\n",
        "\n",
        "    # convert the predicted token ids to tokens and concatenate subwords with \"##\" prefix\n",
        "    predicted_tokens = [remove_subword_prefix(tokenizer.convert_ids_to_tokens(pred)) for pred in predicted_token_indices]\n",
        "\n",
        "    return predicted_tokens, predicted_token_probs\n"
      ],
      "metadata": {
        "id": "nkGSSXAbAj75"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(tokenizer, sentence):\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    return  tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "id": "mSzRUBqMC856"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_embeddings(tokens, tokenizer, model, device=torch.device('cpu')):\n",
        "    \n",
        "    # Get embeddings for the two texts separately\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "      attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]\n",
        "      input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokens)], dtype=torch.long).to(device)\n",
        "\n",
        "      attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "      # Get the output from the classification layer\n",
        "      hidden_state = outputs.last_hidden_state[:, 0, :]\n",
        "      embeddings.append(hidden_state)\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "Mi7XlmjQ-4dk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cosine_similarity(model, tokenizer, original, predicted, device=torch.device('cpu')):\n",
        "\n",
        "    # tokenize sentences\n",
        "    token_original = get_tokens(tokenizer, original)\n",
        "    token_predicted = get_tokens(tokenizer, predicted)\n",
        "\n",
        "    # Get embeddings\n",
        "    embeddings_original = calculate_embeddings(token_original, tokenizer, model, device=device)\n",
        "    embeddings_predicted = calculate_embeddings(token_predicted, tokenizer, model, device=device)\n",
        "    \n",
        "    # Move embeddings to CPU\n",
        "    embeddings_original = torch.cat(embeddings_original).to('cpu').numpy()\n",
        "    embeddings_predicted = torch.cat(embeddings_predicted).to('cpu').numpy()\n",
        "\n",
        "    # Calculate the cosine similarity between the two sets of embeddings\n",
        "    cosine_sim = cosine_similarity(embeddings_original, embeddings_predicted)\n",
        "\n",
        "    # avg_cosine_sim = np.mean(cosine_sim)\n",
        "\n",
        "    return cosine_sim[0, 0]"
      ],
      "metadata": {
        "id": "SpU-rqqB-7mT"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}